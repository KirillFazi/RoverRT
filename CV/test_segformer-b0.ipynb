{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from transformers import pipeline, SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "import requests\n",
    "from PIL import Image\n",
    "import urllib.parse as parse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirill/.pyenv/versions/3.10.11/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "# a function to determine whether a string is a URL or not\n",
    "def is_url(string):\n",
    "    try:\n",
    "        result = parse.urlparse(string)\n",
    "        return all([result.scheme, result.netloc, result.path])\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# a function to load an image\n",
    "def load_image(image_path):\n",
    "    \"\"\"Helper function to load images from their URLs or paths.\"\"\"\n",
    "    if is_url(image_path):\n",
    "        return Image.open(requests.get(image_path, stream=True).raw)\n",
    "    elif os.path.exists(image_path):\n",
    "        return Image.open(image_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "# convert PIL Image to pytorch tensors\n",
    "img_path = \"pictures/5e65508b43b3c10fe35c60e23ab57eb8.jpg\"\n",
    "image = load_image(img_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "image_tensor = image.convert(\"RGB\")\n",
    "image_tensor = transform(image_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "def color_palette():\n",
    "    \"\"\"Color palette to map each class to its corresponding color.\"\"\"\n",
    "    return [[0, 128, 128],\n",
    "            [255, 170, 0],\n",
    "            [161, 19, 46],\n",
    "            [118, 171, 47],\n",
    "            [255, 255, 0],\n",
    "            [84, 170, 127],\n",
    "            [170, 84, 127],\n",
    "            [33, 138, 200],\n",
    "            [255, 84, 0],\n",
    "            [0, 118, 255],\n",
    "            [0, 84, 255],\n",
    "            [0, 0, 255],\n",
    "            [0, 255, 255],\n",
    "            [0, 255, 0]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "def overlay_segments(image, seg_mask):\n",
    "    \"\"\"Return different segments predicted by the model overlaid on image.\"\"\"\n",
    "    H, W = seg_mask.shape\n",
    "    image_mask = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "    colors = np.array(color_palette())\n",
    "    # convert to a pytorch tensor if seg_mask is not one already\n",
    "    seg_mask = seg_mask if torch.is_tensor(seg_mask) else torch.tensor(seg_mask)\n",
    "    unique_labels = torch.unique(seg_mask)\n",
    "    # map each segment label to a unique color\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        image_mask[seg_mask == label.item(), :] = colors[i]\n",
    "    image = np.array(image)\n",
    "    # percentage of original image in the final overlaid iamge\n",
    "    img_weight = 0.5\n",
    "    # overlay input image and the generated segment mask\n",
    "    img = img_weight * np.array(image) * 255 + (1 - img_weight) * image_mask\n",
    "    return img.astype(np.uint8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "def to_tensor(image):\n",
    "    \"\"\"Convert PIL Image to pytorch tensor.\"\"\"\n",
    "    transform = transforms.ToTensor()\n",
    "    image_tensor = image.convert(\"RGB\")\n",
    "    image_tensor = transform(image_tensor)\n",
    "    return image_tensor\n",
    "\n",
    "# a function that takes an image and return the segmented image\n",
    "def get_segmented_image(model, feature_extractor, image_path):\n",
    "    \"\"\"Return the predicted segmentation mask for the input image.\"\"\"\n",
    "    # load the image\n",
    "    image = load_image(image_path)\n",
    "    # preprocess input\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    # convert to pytorch tensor\n",
    "    image_tensor = to_tensor(image)\n",
    "    # pass the processed input to the model\n",
    "    outputs = model(**inputs)\n",
    "    print(\"outputs.logits.shape:\", outputs.logits.shape)\n",
    "    # interpolate output logits to the same shape as the input image\n",
    "    upsampled_logits = F.interpolate(\n",
    "        outputs.logits, # tensor to be interpolated\n",
    "        size=image_tensor.shape[1:], # output size we want\n",
    "        mode='bilinear', # do bilinear interpolation\n",
    "        align_corners=False)\n",
    "\n",
    "    # get the class with max probabilities\n",
    "    segmentation_mask = upsampled_logits.argmax(dim=1)[0]\n",
    "    print(f\"{segmentation_mask.shape=}\")\n",
    "    # get the segmented image\n",
    "    segmented_img = overlay_segments(image_tensor.permute(1, 2, 0), segmentation_mask)\n",
    "    # convert to PIL Image\n",
    "    return Image.fromarray(segmented_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "def replace_label(mask, label):\n",
    "  \"\"\"Replace the segment masks values with label.\"\"\"\n",
    "  mask = np.array(mask)\n",
    "  mask[mask == 255] = label\n",
    "  return mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "img_segmentation_pipeline = pipeline('image-segmentation',\n",
    "                                     model=\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': None,\n  'label': 'road',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>},\n {'score': None,\n  'label': 'sidewalk',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>},\n {'score': None,\n  'label': 'building',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>},\n {'score': None,\n  'label': 'pole',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>},\n {'score': None,\n  'label': 'traffic light',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>},\n {'score': None,\n  'label': 'traffic sign',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>},\n {'score': None,\n  'label': 'vegetation',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>},\n {'score': None,\n  'label': 'sky',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>},\n {'score': None,\n  'label': 'person',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>},\n {'score': None,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>},\n {'score': None,\n  'label': 'truck',\n  'mask': <PIL.Image.Image image mode=L size=1200x800>}]"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = img_segmentation_pipeline(image)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "11"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "# load the feature extractor (to preprocess images) and the model (to get outputs)\n",
    "W, H = image.size\n",
    "segmentation_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "for i in range(len(output)):\n",
    "    segmentation_mask += replace_label(output[i]['mask'], i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "# overlay the predicted segmentation masks on the original image\n",
    "segmented_img = overlay_segments(image_tensor.permute(1, 2, 0), segmentation_mask)\n",
    "\n",
    "# convert to PIL Image\n",
    "converted_img = Image.fromarray(segmented_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "# get_segmented_image(model, feature_extractor, img_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "# show converted image\n",
    "converted_img.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=1200x800>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAAMgCAAAAAAWi8TeAAAMTklEQVR4nO3d0XabShZFUblH//8vqx86cSxLQAEFnF3M+ZQ7nOgKOCwBwtLjAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADT4uvoJcKjn3z+8b+iZH0FRhnVgz6kffP36iSkghFEd12Sv3hkDMvzn6idABSvaBhcSLCCGYAExBAuIIVhADMECYggWEEOweDzc10AIdwwOa22DjAL1OcLij6ejLMrzsjqS/ckxD5RmQAfR8/DIUFCV2RxC97M5c0FJBnMIh1x+MhuUYyiHcNT1cuNBLd4lZIZ3DqnFS+gQDu2KGaEMwziEww+EzAklGMQhnHTmZlq4mBEcwjWXmgwPZ3PRnc1ckudsgsUOisW5BAuIIVjs4RCLUwnW2Vz4gc3+e/UTuI1j7+1UQW7BEdY5fh5XiQtsJFin0CjowSnhCeQK+nCEdTy9gk4E62gf3hVUMNjGKeFer/V5+/U6cYJ+/P7qZpMp+lr6Cwes9Yu7aIw4h0nbZlcgDljpNQ7kTBMHc0q4QY06FPT/FSNbHMZwrdGnVEes83oNNVkcwBFWo3pFKO7PCtMtejJPbTr2amqVL7zdOK92T00ZnRilJsd/Efzn/0Pz5qkdrIdBow9z1OboI6zZx2/YSOWDZdLowTWsAEO8+fZMXwAqEKwSWg6Qno/waD3Dnz8FmKA2B1/EWvHway6BVWTi2MERVpr0U6shzm+5isFpc+wR1spH3/0ANZg9VnOENYLIXuVflON8BqZN1yT8XumrH/z1ATJz9e3fF2gYRpb4AL8LPGf/c+ejxXl++BN85kWtUd+d6cdq3/bA3w8w2E5uHpllQBr1DsNXh4f9Gi1XDwPJPPPRaLw0VGUkmeYaFsV8+NYO+EOwKEeymOL4u5F96GxGk3emopFgXcJ88sIpIZU5O+SFYP1j36joqVn845C7kb3makaVu0zBc/9iClYh95haPrjDpu/yCUyCVc0dRpffbrDV/6Zm36IKVkE3mF5eDb/Jf4Rmz7LqVUnDjy+/jP4u4XPizwzBJr0bnzjawG5RVoe3U0gy+Pbe9fXvnx6CagafYF4NvrlfarNhWdWqvsFHmBdjnxLu641aQTEDvjxNdWblospVigFnmCmjbeyZzKxaVLkKMtoQM22wU0KduaN/3xPmG8NGN9SGXcjVmmVVvnBDDTbfRtmuLYFZsax6FW+UyeZF/ilhe1vcZAjhsoPlQIgpXp6GFBwstYK7if3lZx+cyzwDMqLQYMkVixZmxAwlSjzR3zxo3ia8mY8bvNMnpHGJrGtYMsIa7xfeTVC4oGDtnjWvp/f2PkDeSYwTEywvjaz3XSTjM4iQl5gu8+a3n/ktZP75K+IISzyAxyPjtga94ihmK0xAsHrNlKN/SFd9L+74CugTR/mg+h7Ai8LXsBQDeFU0WGIFvKt5DatCr5wq3EOFWaNZxd3ymBFav6RG+R4q7gNMqHeEVeeX6A0yFFMtWHVy9VCsm6g0ciwoddG93OR8lXtGcGtVDiKOL8O2JVWsO6iyF7CoyCnhhV2QJJ6lrkQwo8ZryxnjMrWkz/l1YJLvo8bOwJwS2+iUKEws6XP6R98/5z5K7BBMKnDR/fok+OBJ/np7AVs4Audcl2+M03L1eUmfcz/88Re4l6/H423bX76vcOVGOLsEs8GaXg+CxV+Kdb0rTgkrNsBZIYsMyfXOv62h1DvIy8/FjEIdJ++P18Vq6Ts1Z257gD+8fF3t1COsUgdXv009NzMKZZx1Datkqko+KQpzFetqx2+AGlX4tJy/n5mTQpas3V8WZkf/1jp6jVXZ2T8s5/tTUyyWNO8wq4ZGuFodfA2ryq7e1CtY1Hoddt14lb66W8qh17BsBcbT8rs66yffbwC1OXAtFcpV4wGW291pdcQvzEvWssPWUal9/H0pPz89F7FYYfHevj6PyA/HrKFiO3jzFSyHWKz065Md+j4eb/qvn3r7dvsVd8Fik6+OEyJZc3qvnYp79oq3CJ0TUoBmTer8LmHFHdstDYRZ+hjcG+u6Wkp2oOUe99m/PP8v4Cwa1nUd1NynV/XKOSHF3T1avU4Jq+7QK3sFtf0b3numq8NSF97/N9woM/PlOlDUfeK1d0lL78ibbuyb/+x3iDBqwnYtV/G9eGN6FItBDFitHYtUfRfeHJ7+v3EB1xguWZsXqPwOvONilCv1jGOsZm1bmoC9d9fFc8ViKONEa/VtDSH77ThbCHYb59b5VUsREqvHY/fdCQ6xGNAAzVpxhJW0ww6wZaC3Z/6e0f6Z7rfqVdLCQrtn+MfHNx5hZS9kH19WAiOIvqDV8sTjdtTphWpeFPdiMbrIaC086chddGaZ2pfHZXfuIK1aR3z5x8X6LJJDLO4iKVozF90H3Dt3LtKAawSivsZ1+qJ70EIAe+Tc7zAVrCFzNeRCQQ8h7x1+DpY9+xNrhbEFHGh9CtaoO+aoywXdVG/We7Ds1nBnpU8Oe3/TdgVdPpb9/UEGWTvQpmS0Bv19k/23fb49wpgrCuaUi9agwXq8r+qVC7rzn8MYiiVr3GD9XtVrF3Tfv4ZhVGrWyMF6WdHrl/P7X4+8iqBFmWYNHayfq3no5YTjlYjW2MH640uuoIfLo3WLYAEdXZgtwQK2uCRbggXscma5BAvo4JxsCRbQzdHZEiygrwOrJVjAcTrHS7CAo3XLlmABp+hRLcECzrOzWoIFnGx7tQQLuMC2aAkWcKk16RIsoIC2bAkWUENDswQLKGOpWYIFVDLbrP+c9SwAGjznDqIEC6hlJlmCBVQzWSzBAsqZKpZgAfVMnBYKFlDRx2S5rQEo6/c9DoIFxHBKCMQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBiCBcQQLCCGYAExBAuIIVhADMECYggWEEOwgBj/A6olewa60VhxAAAAAElFTkSuQmCC"
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['mask']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=1200x800>",
      "image/png": "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"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1]['mask']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "# segmented_image_from_func = get_segmented_image(model, feature_extractor, img_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "# segmented_image_from_func.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "# show only road\n",
    "road = np.zeros((H, W), dtype=np.uint8)\n",
    "road[segmentation_mask == 0] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "side_walk = np.zeros((H, W), dtype=np.uint8)\n",
    "side_walk[segmentation_mask == 1] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "(800, 1200)"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "road.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(road[-100:-1, W//2-100:W//2+100])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def detect_location(mask):\n",
    "    \"\"\"Detect the location of the object in the image.\"\"\"\n",
    "    H, W = mask.shape\n",
    "\n",
    "    detection_cof_h = int(H * 0.05)\n",
    "    detection_cof_w = int(W * 0.05)\n",
    "\n",
    "    detection_area = [-detection_cof_h, -1, W//2-detection_cof_w, W//2+detection_cof_w]\n",
    "\n",
    "    location_label = {\n",
    "        'road': 0,\n",
    "        'side_walk': 0,\n",
    "    }\n",
    "\n",
    "    for i, key in enumerate(location_label):\n",
    "        way = np.zeros((H, W), dtype=np.uint8)\n",
    "        way[mask == i] = 1\n",
    "\n",
    "        location_label[key] = (np.mean(way[detection_area[0]:detection_area[1], detection_area[2]:detection_area[3]]) > 0.8) * 1\n",
    "\n",
    "    return location_label\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "data": {
      "text/plain": "{'road': 1, 'side_walk': 0}"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_location(segmentation_mask)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "road = np.zeros((H, W), dtype=np.uint8)\n",
    "road[segmentation_mask == 0] = 1\n",
    "\n",
    "cof_h = int(H * 0.05)\n",
    "cof_w = int(W * 0.05)\n",
    "\n",
    "center_w = W//2\n",
    "\n",
    "left_dist = []\n",
    "right_dist = []\n",
    "\n",
    "for i in range(-1, -cof_h, -1):\n",
    "    for j in range(center_w, 0, -1):\n",
    "        if road[i, j] == 0:\n",
    "            left_dist.append(center_w - j)\n",
    "            break\n",
    "    left_dist.append(center_w - 0)\n",
    "\n",
    "    for j in range(center_w, W):\n",
    "        if road[i, j] == 0:\n",
    "            right_dist.append(j - center_w)\n",
    "            break\n",
    "    right_dist.append(W - center_w)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "600.0"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(left_dist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "data": {
      "text/plain": "600.0"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(right_dist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
